{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Word List for OCR verification\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "In order to calculate the rough accuracy of the OCR on the SDA periodicals, I created a word bank against which each token in each periodical page could be compared. While this approach does not capture errors of recognition that result in a valid word, it provides a generalized picture of the percentage of each text that is nonsensical and provides an entry point for determining the ways the OCR has failed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:02.200458",
     "start_time": "2017-02-14T17:07:02.196384"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Wordlist against which to evaluate the OCR\n",
    "spelling_dictionary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:02.618817",
     "start_time": "2017-02-14T17:07:02.607653"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for pulling words from a CSV file\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_from_csv(file_name, column_name):\n",
    "    word_list = []\n",
    "    with open(file_name, \"r\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            word_list.append(row[column_name])\n",
    "    word_list = [w.lower() for w in word_list]\n",
    "    return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:03.071035",
     "start_time": "2017-02-14T17:07:03.062870"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for pulling words from a txt file\n",
    "\n",
    "def load_from_txt(file_name):\n",
    "    with open(file_name, \"r\") as txt:\n",
    "        words = txt.read().splitlines()\n",
    "        word_list = [w for w in words]\n",
    "    return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:03.776981",
     "start_time": "2017-02-14T17:07:03.771091"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for getting the unique set of words when adding new list\n",
    "\n",
    "def get_unique_words(word_list, existing_list):\n",
    "    return(set(word_list)-set(existing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:04.198014",
     "start_time": "2017-02-14T17:07:04.194174"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to add to spelling dictionary\n",
    "\n",
    "def add_to_list(word_list, dictionary):\n",
    "    for each in word_list:\n",
    "        dictionary.append(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "There are many word lists that are used in general digital humanities work as a source against which to compare, but very little written on the sources form which those lists are drawn.\n",
    "\n",
    "One of the most common is the NLTK wordlist. This word list is often the default in language processing work. The source of this wordlist and the broad scope of the words it includes, however, makes the dataset problematic for my purposes here. As noted in the README file included with the \"words\" corpus, the words list is the same as the list of words included by default on Unix operating system ([http://en.wikipedia.org/wiki/Words_(Unix)](http://en.wikipedia.org/wiki/Words_(Unix)).\n",
    "\n",
    "This suggests a uniformity in the words list included with all Unix systems. However, the words list included with MacOS Sierra and the list included with Ubuntu 16.04.1 are, in fact, quite different. MacOS uses a word list derrived from the 2nd edition of Websters International Dictionary (according to the README in /usr/share/dict/), this word list is very generous, including all single letters and many uncommon words. Given that our context here is to identify words that are misspelled and are likely OCR errors, this extensive of a list is actually detrimental to our project. A comparison between the two lists reveals that the NLTK word list is, in fact, the list derrived from Websters International Dictionary with 6 additions (see [compare_nltk_to_web2.ipynb](http://localhost:8888/notebooks/drafts/code/compare_nltk_to_web2.ipynb)).\n",
    "\n",
    "Ubuntu, by contrast, relies upon the SCOWL (Spell Checking Oriented Word Lists) package, version 7.1 (2011) for its wordlists ([http://packages.ubuntu.com/source/precise/scowl](http://packages.ubuntu.com/source/precise/scowl)). This package provides a series of wordlists compiled by Kevin Atkinson, broken into different packages for creating and supporting spell-check software. These packages \n",
    "\n",
    "\n",
    "http://app.aspell.net/create?defaults=en_US\n",
    "\n",
    "In addition, because of the rich biblical language with which Seventh-day Adventism understood and expressed their world, I have added a wordlist created from the text of the King James Bible. The process by which the text from the Christian Ethereal Classics Library was converted into a list of words is documented in [/drafts/code/Create_Scriptural_Word_List.ipynb](http://localhost:8000/notebooks/drafts/code/Create_Scriptural_Word_List.ipynb). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the SCOWL Custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:09.670937",
     "start_time": "2017-02-14T17:07:09.635075"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scowl = load_from_txt('/Users/jeriwieringa/Dissertation/drafts/data/word-lists/SCOWL-wl/words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:10.295574",
     "start_time": "2017-02-14T17:07:10.283712"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the SCOWL Custom List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:11.383763",
     "start_time": "2017-02-14T17:07:11.379748"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Identify the abbreviations, as these are not relevant to the SDA corpus.\n",
    "\n",
    "regex_1 = re.compile('\\w[A-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:15.197105",
     "start_time": "2017-02-14T17:07:15.103987"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl2 = [x for x in scowl if not regex_1.match(x)]\n",
    "\n",
    "len(scowl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:15.921098",
     "start_time": "2017-02-14T17:07:15.916370"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " \"A's\",\n",
       " 'Aachen',\n",
       " \"Aachen's\",\n",
       " 'Aalborg',\n",
       " 'Aalesund',\n",
       " 'Aaliyah',\n",
       " \"Aaliyah's\",\n",
       " 'Aalst',\n",
       " \"Aalst's\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:17.028616",
     "start_time": "2017-02-14T17:07:17.000074"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169543"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl3 = [x for x in scowl2 if len(x) > 2]\n",
    "\n",
    "len(scowl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:17.680352",
     "start_time": "2017-02-14T17:07:17.675440"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A's\",\n",
       " 'Aachen',\n",
       " \"Aachen's\",\n",
       " 'Aalborg',\n",
       " 'Aalesund',\n",
       " 'Aaliyah',\n",
       " \"Aaliyah's\",\n",
       " 'Aalst',\n",
       " \"Aalst's\",\n",
       " 'Aalto']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:21.654525",
     "start_time": "2017-02-14T17:07:21.613281"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scowl4 = [x.lower() for x in scowl3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:22.120329",
     "start_time": "2017-02-14T17:07:22.082140"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166169"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(scowl4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:23.163861",
     "start_time": "2017-02-14T17:07:23.085378"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_to_list(list(set(scowl4)), spelling_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:23.762847",
     "start_time": "2017-02-14T17:07:23.757532"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166169"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spelling_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:24.494657",
     "start_time": "2017-02-14T17:07:24.486133"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orienteer',\n",
       " 'olen',\n",
       " 'legree',\n",
       " \"spica's\",\n",
       " 'diageotropism',\n",
       " 'eunuchs',\n",
       " 'measurelessly',\n",
       " \"columbus's\",\n",
       " 'carnotites',\n",
       " 'pyramiding']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding word list created from the KJV translation of the Bible\n",
    "\n",
    "Wordlist created in /drafts/code/Create_Scriptural_Word_List.ipynb from transcription of the KJV Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:30.140979",
     "start_time": "2017-02-14T17:07:30.133134"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biblical_language = load_from_txt(\"/Users/jeriwieringa/Dissertation/drafts/data/word-lists/kjv_bible_wordlist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:31.528507",
     "start_time": "2017-02-14T17:07:31.521011"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14275"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biblical_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:34.616653",
     "start_time": "2017-02-14T17:07:34.612107"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexandrians',\n",
       " 'chastised',\n",
       " 'murdered',\n",
       " 'ezri',\n",
       " 'desire',\n",
       " 'obadiah',\n",
       " 'betonim',\n",
       " 'knocketh',\n",
       " 'disgrace',\n",
       " 'lendest']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biblical_language[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:37.820304",
     "start_time": "2017-02-14T17:07:37.784464"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_biblical_words = get_unique_words(biblical_language, spelling_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:38.499033",
     "start_time": "2017-02-14T17:07:38.492627"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5116"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_biblical_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:39.051131",
     "start_time": "2017-02-14T17:07:39.045887"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hakupha',\n",
       " 'jeshua',\n",
       " 'nebuzar',\n",
       " 'izri',\n",
       " 'hodaviah',\n",
       " 'ephrathites',\n",
       " 'shaashgaz',\n",
       " 'patheus',\n",
       " 'reasoneth',\n",
       " 'phaldaius']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_biblical_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:43.025659",
     "start_time": "2017-02-14T17:07:43.021420"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_to_list(list(unique_biblical_words), spelling_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:45.579864",
     "start_time": "2017-02-14T17:07:45.574899"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171285"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spelling_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:46.349450",
     "start_time": "2017-02-14T17:07:46.343543"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orienteer',\n",
       " 'olen',\n",
       " 'legree',\n",
       " \"spica's\",\n",
       " 'diageotropism',\n",
       " 'eunuchs',\n",
       " 'measurelessly',\n",
       " \"columbus's\",\n",
       " 'carnotites',\n",
       " 'pyramiding']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the word bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:53.434974",
     "start_time": "2017-02-14T17:07:52.146502"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_error_totals(errors_list, all_tokens):\n",
    "    count = 0\n",
    "    freq_distribution = nltk.FreqDist(all_tokens)\n",
    "    for each in errors_list:\n",
    "        frequency = freq_distribution[each]\n",
    "        count = count + int(frequency)\n",
    "    return(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:53.449149",
     "start_time": "2017-02-14T17:07:53.436885"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def check_words(text, file, spell_dictionary):\n",
    "    \n",
    "    text_cleaned = re.sub(r\"[0-9,.!?$\\\"\\(\\)]\", \"\", text)\n",
    "    \n",
    "    '''\n",
    "    Making the choice for spell-check purposes to remove the '-' of hyphenated words. This allows me to check value of \n",
    "    each part of the combined word, without having to expand the dictionary too much. Also allows for greater variability\n",
    "    in the construction of hyphenated words (as was often the case in 19th century writing.)\n",
    "    ''' \n",
    "    text_cleaned = re.sub(r\"[-]\", \" \", text_cleaned)\n",
    "    \n",
    "    tokens = word_tokenize(text_cleaned)\n",
    "    tokens_lower = [w.lower() for w in tokens]\n",
    "    \n",
    "    errors = set(tokens_lower)-set(spelling_dictionary)    \n",
    "    error_total = calculate_error_totals(errors, tokens_lower)\n",
    "    \n",
    "    print(error_total)\n",
    "                          \n",
    "    overview = {'doc_id': file, 'num_tokens' : len(tokens), 'num_errors': error_total, 'errors' : ', '.join(list(errors)) }\n",
    "     \n",
    "    return(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:53.623341",
     "start_time": "2017-02-14T17:07:53.617385"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_process(file):\n",
    "    with open(input_dir + file, \"r\") as f:\n",
    "        print(file)\n",
    "        content = f.read()\n",
    "        print(content)\n",
    "        stats = check_words(content, file, spelling_dictionary)\n",
    "        print(\"Errors: {}\".format(stats['errors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:55.258485",
     "start_time": "2017-02-14T17:07:55.254536"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = '/Users/jeriwieringa/Dissertation/text/text-current/2016-11-16-corpus-with-preliminary-cleaning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:07:56.138511",
     "start_time": "2017-02-14T17:07:56.090978"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV19000601-V02-06-page13.txt\n",
      " existence. Can you afford to postpone the starting of a church school ? If you do not know what has been written concerning Christian education, insist that the subject be presented. If there are church-school teachers on the grounds, question them.\n",
      "Literature on the subject will be for sale, See that you have it all.\n",
      "The words of Jesus are important: Ò Yet a little while is the light with you. Walk while ye have theiight, lest darkness come upon you.Ó\n",
      "ONE might think that Seventh-day Adventists advocated the slow plod ding of the tortoise, and it may be that there are cases when the slow, steady movement will accomplish more than the rapid one; but whether that applies to educational reform or not, is a question. We are told to make a rush for the king dom of heaven ; and while some of those who counsel to Ò move slowly,Ó Ò take time to consider,Ó are arriving at a final decision, the children are growing to ma turity, and not only growing up, but growing away from the hotne and the church.\n",
      "Solomon says there is Ò a time for every purpose under heaven.ÕÕ The time has come to start schools for the children. The Lord has told this in the most positive manner. Those who do not take up this duty now will awake some day to find that the work of warning the world has passed on to an other people. It will go to those who are willing to do an educational work, for the third angelÕs message is an educational re\n",
      "form. There are to-day men of the world who recognize the evils of modern education, and who will take up this work if you let it pass. It is time to have a church school, and to understand why you have one.\n",
      "TIIE ADVOCATE\n",
      "187\n",
      "THE OPPORTUNE TIME.\n",
      "TEACHERSÕ CONFERENCE BULLETIN.\n",
      "AFULL report of the proceedings of the TeachersÕ Conference will be issued under cover of the T r a in in g -\n",
      "School Advocate. Two numbersinJuly\n",
      "and the regular August issue will be\n",
      "devoted to this matter. This will give\n",
      "seventy-five or oue hundred pages of read\n",
      "ing matter on the subject of Christian\n",
      "education, which no one who is interested\n",
      "in the subject can afford to miss. The time\n",
      "of the Conference will be devoted to the\n",
      "discussion of such subjects as Ò Educational\n",
      "work the basis for all Christian growth ; Ó\n",
      "ÔÔ Character and scope of work of the institu\n",
      "tions belonging to the system of Christian\n",
      "education ; Ó Ò Is it possible and practicable\n",
      "for each church to maintain a school ? ÕÕ live matter in convenient form, see that\n",
      "Ò Financial support of church schools;Ó they have the three special numbers of the ÒBooksforChristianschools;Ó ÒChange Advocate, knownastheTeachersÕCon of methods necessary in church schools ; Ó f e r e n c e B u l l e t i n . For price see Review 44Does a public school teacher require a and Herald. Send at once.\n",
      "training in methods of Christian educa tion? Ó and many other subjects of equal importance.\n",
      "Each subject will be opened by a paper. The names of Elder A. T. Jones, Wm. Covert, N. W. Kauble, and Drs. Kellogg, Paulson, Edwards, and Holden, are among those that appear on the program.\n",
      "We believe that these papers, together with a stenographic report of the discus sions, will offer material which will prove inestimable.\n",
      "Subscribers to the A d v o c a te will receive these special numbers without extra charge ; but if you wish others to obtain much in formation on Christian education which is.\n",
      "\n",
      "75\n",
      "Errors: òbooksforchristianschools, teachersõ, ôô, paulson, numbersinjuly, l, consideró, :, hotne, tions, youó, ó, sions, kauble, c, f, r, theiight, educa, institu, afull, õõ, w, tion, d, drs, g, wm, u, tiie, turity, òchange, n, slowlyó, ;, t, v, te, ò, knownastheteachersõcon, oue, heavenõõ, ma, e, angelõs, re\n"
     ]
    }
   ],
   "source": [
    "test_process('ADV19000601-V02-06-page13.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving out the results for use and re-use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:08:07.787260",
     "start_time": "2017-02-14T17:08:07.641658"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "outdir = \"/Users/jeriwieringa/Dissertation/drafts/data/word-lists/\"\n",
    "with open(\"{}{}-Base-Word-List-SCOWL&KJV.txt\".format(outdir, str(datetime.date.today())), 'w') as outfile:\n",
    "    for each in spelling_dictionary:\n",
    "        outfile.write(\"{}\\n\".format(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Info at time of last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-14T17:08:10.259652",
     "start_time": "2017-02-14T17:08:09.058469"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': '5c9c918',\n",
      " 'commit_source': 'installation',\n",
      " 'default_encoding': 'UTF-8',\n",
      " 'ipython_path': '/Users/jeriwieringa/miniconda3/envs/dissertation2/lib/python3.5/site-packages/IPython',\n",
      " 'ipython_version': '5.1.0',\n",
      " 'os_name': 'posix',\n",
      " 'platform': 'Darwin-16.4.0-x86_64-i386-64bit',\n",
      " 'sys_executable': '/Users/jeriwieringa/miniconda3/envs/dissertation2/bin/python',\n",
      " 'sys_platform': 'darwin',\n",
      " 'sys_version': '3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, '\n",
      "                '17:52:12) \\n'\n",
      "                '[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]'}\n",
      "anaconda-client==1.5.5\n",
      "appnope==0.1.0\n",
      "argh==0.26.1\n",
      "beautifulsoup4==4.5.3\n",
      "blinker==1.4\n",
      "bokeh==0.12.4\n",
      "boto==2.43.0\n",
      "bz2file==0.98\n",
      "chest==0.2.3\n",
      "cleanOCR==0.1\n",
      "cloudpickle==0.2.1\n",
      "clyent==1.2.2\n",
      "cycler==0.10.0\n",
      "dask==0.12.0\n",
      "datashader==0.4.0\n",
      "datashape==0.5.2\n",
      "decorator==4.0.10\n",
      "docutils==0.12\n",
      "doit==0.29.0\n",
      "gensim==0.12.4\n",
      "Ghost.py==0.2.3\n",
      "ghp-import2==1.0.1\n",
      "GoH==0.1\n",
      "gspread==0.4.1\n",
      "HeapDict==1.0.0\n",
      "httplib2==0.9.2\n",
      "husl==4.0.3\n",
      "ijson==2.3\n",
      "ipykernel==4.5.2\n",
      "ipython==5.1.0\n",
      "ipython-genutils==0.1.0\n",
      "ipywidgets==5.2.2\n",
      "Jinja2==2.8\n",
      "jsonschema==2.5.1\n",
      "jupyter==1.0.0\n",
      "jupyter-client==4.4.0\n",
      "jupyter-console==5.0.0\n",
      "jupyter-contrib-core==0.3.0\n",
      "jupyter-contrib-nbextensions==0.2.2\n",
      "jupyter-core==4.2.1\n",
      "jupyter-highlight-selected-word==0.0.5\n",
      "jupyter-latex-envs==1.3.5.4\n",
      "jupyter-nbextensions-configurator==0.2.3\n",
      "llvmlite==0.14.0\n",
      "locket==0.2.0\n",
      "Logbook==1.0.0\n",
      "lxml==3.5.0\n",
      "MacFSEvents==0.7\n",
      "Mako==1.0.4\n",
      "Markdown==2.6.7\n",
      "MarkupSafe==0.23\n",
      "matplotlib==2.0.0\n",
      "memory-profiler==0.43\n",
      "mistune==0.7.3\n",
      "multipledispatch==0.4.9\n",
      "natsort==4.0.4\n",
      "nb-anacondacloud==1.2.0\n",
      "nb-conda==2.0.0\n",
      "nb-conda-kernels==2.0.0\n",
      "nb-config-manager==0.1.3\n",
      "nbbrowserpdf==0.2.1\n",
      "nbconvert==4.2.0\n",
      "nbformat==4.2.0\n",
      "nbpresent==3.0.2\n",
      "networkx==1.11\n",
      "Nikola==7.7.7\n",
      "nltk==3.2.2\n",
      "notebook==4.2.3\n",
      "numba==0.29.0\n",
      "numpy==1.12.0\n",
      "oauth2client==4.0.0\n",
      "OCRreports==0.1\n",
      "odo==0.5.0\n",
      "pandas==0.19.2\n",
      "partd==0.3.6\n",
      "path.py==0.0.0\n",
      "pathtools==0.1.2\n",
      "pexpect==4.0.1\n",
      "pickleshare==0.7.4\n",
      "Pillow==3.4.2\n",
      "prompt-toolkit==1.0.9\n",
      "psutil==4.3.0\n",
      "ptyprocess==0.5.1\n",
      "pyasn1==0.1.9\n",
      "pyasn1-modules==0.0.8\n",
      "pycrypto==2.6.1\n",
      "Pygments==2.1.3\n",
      "pyparsing==2.1.10\n",
      "PyPDF2==1.25.1\n",
      "PyRSS2Gen==1.1\n",
      "pyshp==1.2.10\n",
      "python-dateutil==2.6.0\n",
      "pytz==2016.10\n",
      "PyYAML==3.12\n",
      "pyzmq==16.0.2\n",
      "qtconsole==4.2.1\n",
      "requests==2.12.3\n",
      "rsa==3.4.2\n",
      "scipy==0.18.1\n",
      "seaborn==0.7.1\n",
      "simplegeneric==0.8.1\n",
      "six==1.10.0\n",
      "smart-open==1.3.5\n",
      "terminado==0.6\n",
      "textblob==0.11.1\n",
      "toolz==0.8.1\n",
      "tornado==4.4.2\n",
      "traitlets==4.3.1\n",
      "Unidecode==0.4.19\n",
      "verifyOCR==0.1\n",
      "watchdog==0.8.3\n",
      "wcwidth==0.1.7\n",
      "webassets==0.11.1\n",
      "widgetsnbextension==1.2.6\n",
      "ws4py==0.3.4\n",
      "xarray==0.8.2\n",
      "Yapsy==1.11.223\n"
     ]
    }
   ],
   "source": [
    "# %load shared_elements/system_info.py\n",
    "import IPython\n",
    "print (IPython.sys_info())\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "date": "2018-04-01",
   "slug": "create_wordlist",
   "title": "Create SpellCheck Wordlists"
  },
  "toc": {
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
