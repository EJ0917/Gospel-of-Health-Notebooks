{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first challenge was gathering a copy the available documents within the parameters (geographic and temporal) of my study. \n",
    "\n",
    "The source base for my study is the collection of scanned periodicals made available by the [Office of Archives, Statistics, and Research of the Seventh-day Adventist Church](http://documents.adventistarchives.org/). One of the advantages of working with these sources is that they are openly available on the web. This removes the need to navigate through the firewalls (and legal land-mines) of using text from major library databases, a major boon for the digital project. And, although the site does not provide an API for accessing the documents, the structure of the pages is regular, which makes the site a good candidate for web scraping. \n",
    "\n",
    "To determine the list of titles that applied to my time and regions of study, I browsed through all of the titles in the [periodicals section of the site](http://documents.adventistarchives.org/Periodicals/Forms/AllFolders.aspx) and compiled a list of titles that fit my geographic and temporal constraints. These are: \n",
    "\n",
    "* [Training School Advocate (ADV)](http://documents.adventistarchives.org/Periodicals/ADV)\n",
    "* [American Sentinel (AmSn)](http://documents.adventistarchives.org/Periodicals/AmSn)\n",
    "* [Advent Review and Sabbath Herald (ARAI)](http://documents.adventistarchives.org/Periodicals/ARAI)\n",
    "* [Christian Education (CE)](http://documents.adventistarchives.org/Periodicals/CE)\n",
    "* [Welcome Visitor (Columbia Union Visitor) (CUV)](http://documents.adventistarchives.org/Periodicals/CUV)\n",
    "* [Christian Educator (EDU)](http://documents.adventistarchives.org/Periodicals/EDU)\n",
    "* [General Conference Bulletin (GCB)](http://documents.adventistarchives.org/Periodicals/GCSessionBulletins)\n",
    "* [Gospel Herald (GH)](http://documents.adventistarchives.org/Periodicals/GH)\n",
    "* [Gospel of Health (GOH)](http://documents.adventistarchives.org/Periodicals/GOH)\n",
    "* [Gospel Sickle (GS)](http://documents.adventistarchives.org/Periodicals/GS)\n",
    "* [Home Missionary (HM)](http://documents.adventistarchives.org/Periodicals/HM)\n",
    "* [Health Reformer (HR)](http://documents.adventistarchives.org/Periodicals/HR)\n",
    "* [Indiana Reporter (IR)](http://documents.adventistarchives.org/Periodicals/IR)\n",
    "* [Life Boat (LB)](http://documents.adventistarchives.org/Periodicals/LB)\n",
    "* [Life and Health (LH)](http://documents.adventistarchives.org/Periodicals/LH)\n",
    "* [Liberty (LibM)](http://documents.adventistarchives.org/Periodicals/LibM)\n",
    "* [Lake Union Herald (LUH)](http://documents.adventistarchives.org/Periodicals/LUH)\n",
    "* [North Michigan News Sheet (NMN)](http://documents.adventistarchives.org/Periodicals/NMN)\n",
    "* [Pacific Health Journal and Temperance Advocate (PHJ)](http://documents.adventistarchives.org/Periodicals/PHJ)\n",
    "* [Present Truth (Advent Review) (PT-AR)](http://documents.adventistarchives.org/Periodicals/PT-AR) (renamed to PTAR)\n",
    "* [Pacific Union Recorder (PUR)](http://documents.adventistarchives.org/Periodicals/PUR)\n",
    "* [Review and Herald (RH)](http://documents.adventistarchives.org/Periodicals/RH)\n",
    "* [Sabbath School Quarterly (SSQ)](http://documents.adventistarchives.org/SSQ)\n",
    "* [Sligonian (Sligo)](http://documents.adventistarchives.org/Periodicals/Sligo)\n",
    "* [Sentinel of Liberty (SOL)](http://documents.adventistarchives.org/Periodicals/SOL)\n",
    "* [Signs of the Times (ST)](http://documents.adventistarchives.org/Periodicals/ST)\n",
    "* [Report of Progress, Southern Union Conference (SUW)](http://documents.adventistarchives.org/Periodicals/SUW)\n",
    "* [The Church Officer's Gazette (TCOG)](http://documents.adventistarchives.org/Periodicals/TCOG)\n",
    "* [The Missionary Magazine (TMM)](http://documents.adventistarchives.org/Periodicals/TMM)\n",
    "* [West Michigan Herald (WMH)](http://documents.adventistarchives.org/Periodicals/WMH)\n",
    "* [Youth's Instructor (YI)](http://documents.adventistarchives.org/Periodicals/YI)\n",
    "\n",
    "As this was my first technical task for the dissertation, my initial methods for identifying the URLs to the documents I wanted to download was rather manual. I saved a .html file for each web page that contained documents I wanted to download. I then passed those .html files to a script (similar to that recorded here) that used `BeautifulSoup` to extract the PDF ids, reconstruct the URLs, and write the URLs to a new text file. After manually deleting the URLs to any documents that were out of range, I then passed the file with URLs to `wget` using the following syntax: \n",
    "\n",
    "```bash\n",
    "wget -i scrapeList.txt -w 2 --limit-rate=200k\n",
    "```\n",
    "\n",
    "I ran this process for each of the periodical titles included in this study. It took approximately a week to download all 13,000 files to my local machine.\n",
    "\n",
    "This notebook reflects a more automated version of that process, created in 2017 to verify to download any missing documents. The example recorded here is for downloading the Sabbath School Quarterly collection, which I missed during my initial collection phase. \n",
    "\n",
    "These scripts use the [`requests`](http://docs.python-requests.org/en/master/) library to retrieve the HTML from the document directory pages and [`BeautifulSoup4`](https://www.crummy.com/software/BeautifulSoup/) to locate the filenames. I am using [`wget`](https://pypi.python.org/pypi/wget) to download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:40:19.404267Z",
     "start_time": "2017-04-23T23:40:19.216832Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os.path import join\n",
    "import re\n",
    "import requests\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:40:19.905778Z",
     "start_time": "2017-04-23T23:40:19.840365Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_html_page(url):\n",
    "    \"\"\"Use the requests library to get HTML content from URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of webpage with content to download.\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def filename_from_html(content):\n",
    "    \"\"\"Use Beautiful Soup to extract the PDF ids from the HTML page. \n",
    "\n",
    "    This script is customized to the structure of the archive pages at\n",
    "    http://documents.adventistarchives.org/Periodicals/Forms/AllFolders.aspx.\n",
    "\n",
    "    Args:\n",
    "        content (str): Content is retrieved from a URL using the `get_html_page` \n",
    "            function.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    buttons = soup.find_all('td', class_=\"ms-vb-title\")\n",
    "\n",
    "    pdfIDArray = []\n",
    "\n",
    "    for each in buttons:\n",
    "        links = each.find('a')\n",
    "        pdfID = links.get_text()\n",
    "        pdfIDArray.append(pdfID)\n",
    "\n",
    "    return pdfIDArray\n",
    "\n",
    "\n",
    "def check_year(pdfID):\n",
    "    \"\"\"Use regex to check the year from the PDF filename.\n",
    "\n",
    "    Args:\n",
    "        pdfID (str): The filename of the PDF object, formatted as \n",
    "            PREFIXYYYYMMDD-V00-00\n",
    "    \"\"\"\n",
    "    split_title = pdfID.split('-')\n",
    "    title_date = split_title[0]\n",
    "    date = re.findall(r'[0-9]+', title_date)\n",
    "    year = date[0][:4]\n",
    "    if int(year) < 1921:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to set the directory where I want to download the documents to, as well as the root URL for where the PDF documents could be found. The current setup requires the baseurl to be manually changed for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:41:09.501179Z",
     "start_time": "2017-04-23T23:41:09.495215Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_directory = \"\"\n",
    "baseurl = \"http://documents.adventistarchives.org/SSQ/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next step when downloading a collection of documents was to generate a list of the files that I wanted to download. Here I use two functions within the `compile` module to request the HTML from the index_url and to extract the document ids from the HTML. Finally, to avoid downloading any files outside of my study, I check the year in the doc ID before adding it to my list of documents to download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:40:27.400687Z",
     "start_time": "2017-04-23T23:40:27.396792Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_page_urls = [\"http://documents.adventistarchives.org/SSQ/Forms/AllItems.aspx?View={44c9b385-7638-47af-ba03-cddf16ec3a94}&SortField=DateTag&SortDir=Asc\",\n",
    "              \"http://documents.adventistarchives.org/SSQ/Forms/AllItems.aspx?Paged=TRUE&p_SortBehavior=0&p_DateTag=1912-10-01&p_FileLeafRef=SS19121001-04%2epdf&p_ID=457&PageFirstRow=101&SortField=DateTag&SortDir=Asc&&View={44C9B385-7638-47AF-BA03-CDDF16EC3A94}\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:40:29.766200Z",
     "start_time": "2017-04-23T23:40:28.254653Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_to_download = []\n",
    "\n",
    "for url in index_page_urls: \n",
    "    content = get_html_page(url)\n",
    "    pdfs = filename_from_html(content)\n",
    "    \n",
    "    for pdf in pdfs:\n",
    "        if check_year(pdf):\n",
    "            print(\"Adding {} to download list\".format(pdf))\n",
    "            docs_to_download.append(pdf)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I loop through all of the filenames, create the URL to the PDF, and use `wget` to download a copy of the document into my directory for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-23T23:41:20.467678Z",
     "start_time": "2017-04-23T23:41:17.264995Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc_name in docs_to_download:\n",
    "    url = join(baseurl, \"{}.pdf\".format(doc_name))\n",
    "    print(url)\n",
    "    wget.download(url, download_directory)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
